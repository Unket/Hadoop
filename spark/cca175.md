# CCA 175

# Packages Used

```
pyspark --packages com.databricks:spark-csv_2.11:1.5.0
```
```
from pyspark.sql.types import *
```

## Data Ingest


### Import data from a MySQL database into HDFS using Sqoop

```sh

sqoop import --connect jdbc:mysql//localhost/pyspark --username root --password Srianjaneyam@34 --table customer -m 1 --target-dir /user/kaushik_amaravadi/customer_table
```

### Export data to a MySQL database from HDFS using Sqoop

```sh

sqoop export --connect jdbc:mysql://localhost/pyspark --username root --password Srianjaneyam@34 --table customer -m 1 --export-dir /user/kaushik/customer_table 
```

### Change the delimiter and file format of data during import using Sqoop

```sh

sqoop import --connect jdbc:mysql://localhost/pyspark --username root --password Srianjaneyam@34 --table customer -m 1 --fields-terminated-by '\t' --lines-terminated-by '\n' --target-dir /user/kaushik_amaravadi/

### Ingest real-time and near-real-time streaming data into HDFS

## See Flume

### Process streaming data as it is loaded onto the cluster

## See Streaming

### Load data into and out of HDFS using the Hadoop File System commands

## See Commands

## Transform, Stage, and Store



### Load RDD data from HDFS for use in Spark applications

```python
df = sc.textFile('/user/kaushik/Vote.txt').map(lambda l: l.split('\t'))
```
```python
sc.parallelize(df.takeSample(False,7)).saveAsTextFile('/user/kaushik/pyspark/takesevenseven.txt')
```
```python
sc.parallelize(df.take(7)).saveAsTextFile('/user/kaushik/pyspark/takeseven1st.txt')
```
```python
data = df.toDF()
```
```python
data.select('_1').show()
```
```python
dd = data.select('_1','_2','_3','_4','_5').take(7)
```
```python
df = sc.textFile('/user/kaushik/Vote.txt').map(lambda l: l.split('\t')).map(lambda l:(l[0],l[1]))
```
```python
df.first()
```
```
(u'1', u'1')
```

### Write the results from an RDD back into HDFS using Spark

```python
df = saveAstextFile("path")
```
```python
saveAsParquetFile(df, "/tmp/temp.parquet")
```

### Read and write files in a variety of file formats


### Perform standard extract, transform, load (ETL) processes on data

```python
item = sqlContext.read.format('com.databricks.spark.csv').option('header','true').option('inferschema','true').option('delimiter','\t').load('/user/kaushik/sparkjoin/Lokad_Items.tsv')
```
```python

sevencolumns=item.select(item['Id'],item['LabelName'],item['TagLabelCategory'],item['TagSubCategory'],item['ServiceLevel'],item['StockOnOrder'],item['StockOnHand'])

```

## Data Analysis

### Use metastore tables as an input source or an output sink for Spark applications

```python
sqlContext.sql('show databases').show()
```
```python
sqlContext.sql('use ebay')
```
```python
amount = sqlContext.sql('select * from final where Quantity>100 and Quantity<200 limit 10')
```
```python
amount.show()
```
```python
amount.write.format('com.databricks.spark.csv').save('/user/kaushik/quantity')
```

### Understand the fundamentals of querying datasets in Spark


#### on the filtered data set find out the higest value in the product_price column under each category

```python
order.groupBy('Id').agg({'Quantity':'max'}).show()
```

#### on the filtered data set also find out total products under each category

```python
order.groupBy('Id').agg({'Quantity':'count'}).show()
```

#### on the filtered data set also find out the average price of the product under each category

```python
order.groupBy('Id').agg({'Quantity':'avg'}).show()
```

#### on the filtered data set also find out the minimum price of the product under each category

```python
order.groupBy('Id').agg({'Quantity':'min'}).show()
```

### Filter data using Spark
```python
order.filter("order['amount']>10.00").show()
```

### Write queries that calculate aggregate statistics

```sql
select max(Quantity) as a, Id from order group by id order by a
```
#### sort by id ascending order

order.groupBy('Id').agg({'Quantity':'min'}).orderBy(order['Id'].asc()).show()

### Join disparate datasets using Spark

```python
item = sqlContext.read.format('com.databricks.spark.csv').option('header','true').option('inferschema','true').option('delimiter','\t').load('/user/kaushik/sparkjoin/Lokad_Items.tsv')
```
```python
order = sqlContext.read.format('com.databricks.spark.csv').option('header','true').option('inferschema','true').option('delimiter','\t').load('/user/kaushik/sparkjoin/Lokad_Orders.tsv')
```
```python
df = item.join(order,['Id'])
```
```python
df.show()
```
```python
fianl = df.select(df['Id'],df['LabelName'],df['Quantity'])
```
```python
fianl.registerTempTable('final')
```
```python
thee = sqlContext.sql('select Quantity,concat(Id,LabelName) as Full from final')
```
```python

thee.show()
```
### Produce ranked or sorted data

```python
order.groupBy('Id').agg({'Quantity':'min'}).orderBy(order['Id'].asc()).show()
```

### Avro Compression - uncompressed
```
sqlContext.setConf('spark.sql.avro.compression.codec','uncompressed')
```
```
order.write.format('com.databricks.spark.avro').save('/user/kaushik/avro')
```
### Parquet Snappy Compression

```
sqlContext.setConf('spark.sql.parquet.compression.codec','snappy')
```
### Parquet Uncompressed
```
sqlContext.setConf('spark.sql.parquet.compression.codec','uncompressed')
```
### Csv compression 
```
org.apache.hadoop.io.compress.SnappyCodec
```
